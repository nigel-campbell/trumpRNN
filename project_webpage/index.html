<!DOCTYPE html>
<html lang="en"><head>  
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <title>Deep Learning Class Project
  | Georgia Tech | Spring 2019: CS 7643</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="">
  <meta name="author" content="">

<!-- Le styles -->  
  <link href="css/bootstrap.css" rel="stylesheet">
<style>
body {
padding-top: 60px; /* 60px to make the container go all the way to the bottom of the topbar */
}
.vis {
color: #3366CC;
}
.data {
color: #FF9900;
}
</style>
  
<link href="css/bootstrap-responsive.min.css" rel="stylesheet">
</head>

<body>
<div class="container">
<div class="page-header">

<!-- Title and Name --> 
<h1>TrumpRNN: Look-Ahead, Multi-Loss Generative Language Models </h1> 
<span style="font-size: 20px; line-height: 1.5em;"><strong>Bryan Baek, Nigel Campbell</strong></span><br>
<span style="font-size: 18px; line-height: 1.5em;">Spring 2019 CS 7643 Deep Learning: Class Project</span><br>
<span style="font-size: 18px; line-height: 1.5em;">Georgia Tech</span>
<hr>

<!-- Goal -->
<h2>Abstract</h2>
This project employed various language models to capture a style of a well-known figure: Donald Trump. Trump's tweets from 2013 to 2019 were trained via n-gram, character-level RNN architecture with GRU, and word-level RNN architecture with GRU. Next, word-level RNN's loss function was modified to lookahead mechanism and better retain contextual information and word-level dependencies. Finally, generated outputs from these models were qualitatively evaluated. Qualitatively, character level models are best at capturing character level dependencies in Trump Tweets, and word level models are better at maintaining long term word level dependencies in terms of maintaining coherent tweets.

<br><br>
<!-- figure -->
<h2>Teaser figure</h2>
A figure that conveys the main idea behind the project or the main application being addressed. (This one is from <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks">AlexNet</a>.)

<br><br>

<!-- Introduction -->
<h2>Introduction / Background / Motivation</h2>
Recurrent neural networks have been used successfully across a variety of applications ranging from image captioning, machine translation and text generation. In the realm of probabilistic classification, language modelling describes the problem of computing the probability of a sequence of character or word tokens given a discrete vocabulary. This problem has applications across machine translation, speech recognition, summarization and dialogue systems.
<br><br>
Before deep learning, text generation was done using probabilistic Markov Chain Monte Carlo and bag of words model in closed vocabulary settings. However, some of the limits with respect involve maintaining contextual information for long sequences of generated text as well as keeping word dependencies such that generated text remains coherent.
<br><br>
There have been attempts to use deep learning to generate similar text output. Karparthy used a RNN structure to mimic corpus like Shakespeare text, Wikipedia article, a C code, and even a LaTex file [1]. However, while they “look” like the original text, they do not sound like the author of the original document. In “Applying Artistic Style Transfer to Natural Language,” [2] Edirisooriya and Tenney embed IDs and apply RNNs of gated recurrent unit (GRU) to identify authors of a given text based on style. They try to separate the loss function in terms of content and style. However, the outcome of their experiments showed limited results, and this required parallel sets of text data. In Feb 2019, researchers from OpenAI trained a large-scale unsupervised language model with 1.5 billion parameters trained on 40GB of Internet text. that creates coherent paragraphs of text based on a human-input text. [3] While the generated text was remarkably relevant to the input text in terms of content, it lacks similarity in style. 
<div style="text-align: center;">
<img style="height: 200px;" alt="" src="images/GRU.png">
<h5>Figure A: Basic architecture of GRU cell</h5>
</div>

In this work, we explore the space of language modelling techniques and present an application of a recurrent neural network (RNN) based language model for generating text in the style of Donald Trump’s tweets. The current president of the United States Donald J. Trump is an ideal candidate to apply this approach because not only does he have a unique, colloquial tone that is both iconic but also there is a great wealth of data thanks to his frequent tweeting.
<br><br>
This is interesting for the following reasons. First, it not only reveals how a machine learn to speak but also provokes thinking about how we humans grasp the concept of language and speak. Second, from an user’s point of view, this presents an interesting perspective of how the user sounds. Finally, it metaphysically touches on the concept of "immortality". Currently, there exists voice question-and-answer system like Alexa or Watson that can reasonably carry out a conversation. However, they lack in an specific tone that usually defines a person. By combining human-level intellect with a specific person's tone, we could reasonably mimic the person, and this idea would be like "deep-copying" someone's mental features.
<br><br>
<!-- Approach -->
<h2>Approach</h2>
Our dataset consists of tweets from Donald Trump’s official Twitter account <a href="https://twitter.com/realDonaldTrump">@realDonaldTrump</a> from 2013 to 2019, excluding retweets. Combined, the dataset is about 3MB.

<br><br>
<!-- Main Illustrative Figure -->
<div style="text-align: center;">
<img style="height: 200px;" alt="" src="images/trumpTweet.png">
<h5>Exhibit A: Examples of @realDonaldTrump’s tweets</h5>
</div>

For the initial qualitative evaluation, we first built initial closed vocabulary language models using n-grams, char-level GRU-based RNN models, and word-level GRU-based RNN models. N-Gram is a very simple language model that approximates probability of a word given previous N words. N-gram, thus, relies on the Markov assumption, and ignores distant history in a sequence of words. Here, we used n = (2,3,4).
 <br><br>
Thus, we then trained a char-level and word-level GRU-based RNNs using PyTorch Framework. Adding recurrence in the model provides additional contextual information that allows picking up on a larger pattern in the training data. We attempt to model the probability distribution of the next character or word in the sequence given a sequence of previous characters or words, letting us generate the new text one character or word at a time. For both RNN models, we first encode the texts into feature representations (e.g. number of ASCII characters for char level), and run them through two layers of GRU. Finally, decode them into a vector output via a single densely connected output layer with softmax activation function (Figure B). We focused on tuning the following hyperparameters: number of epochs, hidden size of GRU, learning rate, length of training chunks, and batch size.
<div style="text-align: center;">
<img style="height: 200px;" alt="" src="images/RNNArchitecture.png">
<h5>Figure A: Sample of Overall RNN Architecture for Char and Word Level Model</h5>
</div>
From our initial set of models, we aimed to improve the quality of word-level dependencies between long running sequences of generated texts. We anticipated given the small dataset and rather simple nature of the architecture, generated output may not be qualitatively good. From the initial trial runs, we saw that trained models mostly generated gibberish, and we had to cherry pick snip bits of sensible texts. 
 <br><br>
Therefore, we modified the loss function for the word-level model. The loss is computed between multiple sets of sources and “lookahead” targets as opposed to single discrete sets of sources and targets. We hoped this would help retain contextual information and word-level dependencies while remaining computationally tractable. With this modified loss function, at training we look at multiple sources within a given sequence and define multiple targets so that long running dependencies are learned. We looked at the source code provided by Karparthy in his blog, and made modifications to use the PyTorch framework, and added custom loss function as described. Finally, for all of the models, we captured the loss history over the epochs, and generated a synthetic text based on random initial seeds of characters or words.

<br><br>
<!-- Experiment and Results -->
<h2>Experiments and Results</h2>

<br><br>
<!-- Main Illustrative Figure -->
<div style="text-align: center;">
<img style="height: 800px;" alt="" src="images/SampleTweets.png">
<h5>Figure B: Generated Text Output </h5>
</div>

For all of the experiments, we had to measure the success based on the loss history and qualitative evaluation of the generated tweets. On the trained models, we generated the tweets with string primer. With the basic n-grams, with n=2, the generated tweets were semi-coherent, while with n=4, the generated tweet were almost identical to the actual tweets. For RNN models, we had to cherry pick parts of the output that were interesting; this part was also observed and discussed in [1] and [2]. As shown by Figure 2, all of the models are converging, indicating the model is learning something. However, the char level models took about 700 iterations to converge, while word level models took about 10 iterations. This is as expected because word vector is bigger than the character vector.
<div style="text-align: center;">
<img style="height: 200px;" alt="" src="images/LossHistory.png">
<h5>Figure C: Loss History of Char-Level, Word-Level, and modified Loss Function Word-Level RNN </h5>
</div>


<!-- Experiment Plan -->
<h2>Conclusions</h2>
Trump’s style is particularly unique because of his usage of hashtags, capitalizations, and punctuations. We noticed that there was a tradeoff between style and content similarity: char level model would better capture the style because it captures the granularity like the punctuation mark, but the word level model is better at capturing the diction and word choices. While the models are definitely learning, it is still limited. As shown by related works in the area, a much larger text dataset in the order of GBs will improve the performance. 


<br><br>

<!-- Team Member Identification -->
<h2>Team Member Identification</h2>
<table style="width:100%">
  <tr>
    <th>Name</th>
    <th>Description of Work</th>
  </tr>
  <tr>
    <td>Bryan Baek</td>
    <td>Gathered the data, implemented the char level RNN with GRU models, 
    trained and monitored the optimization addressing any issues such as overfitting, tuned hyper-parameters, wrote the report</td>
  </tr>
  <tr>
    <td>Nigel Campbell</td>
    <td>Implemented the word level RNN with GRU models and n-gram models, implemented new loss function, designed and implemented attentional mechanism for approach.</td>
  </tr>
</table>

<br><br>

<!-- References -->
<h2>References</h2>
[1] 	A. Karpathy, "The Unreasonable Effectiveness of Recurrent Neural Networks," 21 May 2015. [Online]. Available: http://karpathy.github.io/2015/05/21/rnn-effectiveness/. [Accessed 28 April 2019].
<br><br>
[2] 	T. Edirisooriya and M. Tenney, "Applying Artistic Style Transfer to Natural Language," 2018. [Online]. Available: http://web.stanford.edu/class/cs224s/reports/Thaminda_Edirisooriya.pdf. [Accessed 28 April 2019].
<br><br>
[3] 	A. Radford, J. Wu, R. Child, D. Luan, D. Amodei and I. Sutskever, "Language Models are Unsupervised Multitask Learners," 2019.
<br><br>
[4] 	T. Shen, T. Lei, R. Barzilay and T. Jaakkola, "Style Transfer from Non-Parallel Text by Cross-Alignment," CoRR, vol. abs/1705.09655, 2017. 
<br><br>


  <hr>
  <footer> 
  <p>© Bryan Baek, Nigel Campbell</p>
  </footer>
</div>
</div>

<br><br>

</body></html>
